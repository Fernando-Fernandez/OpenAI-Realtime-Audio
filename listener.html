<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Realtime Audio Communication to LLM</title>
  <style>
    body { font-family: sans-serif; padding: 20px; }
    #panels { display: flex; gap: 16px; align-items: flex-start; }
    #log, #events { flex: 1; min-height: 240px; white-space: pre-wrap; background: #f0f0f0; padding: 10px; border-radius: 4px; overflow-y: auto; }
    #log { background: #f7f7f7; max-height: 70vh; }
    #events { background: #eef5ff; font-size: xx-small; max-height: 70vh; }
    #session-controls { display: flex; align-items: center; gap: 8px; flex-wrap: wrap; margin-bottom: 12px; }
    #mic-indicator-wrapper { display: inline-flex; align-items: center; gap: 8px; font-size: 0.9em; color: #444; }
    #mic-indicator { width: 96px; height: 32px; display: flex; align-items: flex-end; gap: 4px; padding: 6px 8px 4px; border: 1px solid #d0d0d0; border-radius: 6px; background: #fff; transition: border-color 0.15s, box-shadow 0.15s; }
    #mic-indicator[data-active="true"] { border-color: #2563eb; box-shadow: 0 0 0 2px rgba(37, 99, 235, 0.15); }
    #mic-indicator span { flex: 1; display: block; width: 6px; border-radius: 3px; background: linear-gradient(180deg, #99c2ff, #2563eb); height: 10%; opacity: 0.4; transition: height 0.08s ease-out, opacity 0.1s; }
    #mic-indicator[data-active="true"] span { opacity: 0.9; }
  </style>
</head>
<body>

<h1>Realtime Audio Communication to LLM</h1>

<div id="session-controls">
  <label>OpenAI API Key</label>
  <input id="token" type="password" style="width: 300px;">
  <button id="start">Start</button>
  <button id="stop">Stop</button>
  <label style="display:inline-flex;align-items:center;gap:6px;margin-left:12px;">
    <input type="checkbox" id="enable-audio" checked>
    Enable audio responses
  </label>
  <div id="mic-indicator-wrapper">
    <span>Mic activity</span>
    <div id="mic-indicator" aria-hidden="true"></div>
  </div>
  <label style="display:inline-flex;align-items:center;gap:6px;">
    Voice
    <select id="voice-select">
      <option value="alloy" selected>alloy</option>
      <option value="ash">ash</option>
      <option value="ballad">ballad</option>
      <option value="coral">coral</option>
      <option value="echo">echo</option>
      <option value="fable">fable</option>
      <option value="onyx">onyx</option>
      <option value="nova">nova</option>
      <option value="sage">sage</option>
      <option value="shimmer">shimmer</option>
      <option value="verse">verse</option>
    </select>
  </label>
</div>

<div id="panels">
  <div style="flex:1">
    <h2>Responses</h2>
    <div id="log"></div>
  </div>
  <div style="flex:1">
    <h2>Events</h2>
    <div id="events"></div>
    <div id="rate-limit-timers" style="margin-top: 8px; font-size: 0.9em;"></div>
  </div>
</div>

<script type="module">

const MIN_REQUEST_INTERVAL_MS = 1500
const SPEECH_THRESHOLD = 0.02
const SILENCE_TIMEOUT_MS = 5000
const MIC_BAR_COUNT = 10
let pc = null
let dc = null
let stream = null
let awaitingResponse = false
let responseHasText = false
let currentResponseId = null
let retryTimer = null
let sessionReady = false
let hasRecentSpeech = false
let lastSpeechTs = 0
let lastRequestTs = 0
let audioContext = null
let analyser = null
let analyserBuffer = null
let monitorHandle = null
const rateLimitBackoffs = {}
const latestRateLimits = {}
const conversationItemIds = new Set()
let transcriptReady = false
let audioTranscriptBuffer = ""
let rateLimitTimers = {}
const audioToggle = () => document.getElementById("enable-audio")
const voiceSelect = () => document.getElementById("voice-select")
let remoteAudioEl = null
let remoteStream = null
let micIndicatorBars = []
const QUIET_EVENT_TYPES = new Set([
  "response.output_text.delta",
  "response.output_text.done",
  "response.text.delta",
  "response.text.done",
  "response.created",
  "response.completed",
  "response.done",
  "conversation.item.input_audio_transcription.completed",
  "response.audio_transcript.delta",
  "response.audio_transcript.done"
])

function initMicIndicator() {
  const container = document.getElementById("mic-indicator")
  if (!container) return
  if (container.childElementCount === 0) {
    for (let i = 0; i < MIC_BAR_COUNT; i++) {
      const bar = document.createElement("span")
      container.appendChild(bar)
    }
  }
  micIndicatorBars = Array.from(container.querySelectorAll("#mic-indicator span"))
  setMicIndicatorLevel(0)
}

function setMicIndicatorLevel(level) {
  const container = document.getElementById("mic-indicator")
  if (!container || micIndicatorBars.length === 0) return
  const clamped = Math.max(0, Math.min(1, level))
  micIndicatorBars.forEach((bar, idx) => {
    const bias = 0.5 + (idx / Math.max(1, micIndicatorBars.length - 1)) * 0.6
    const height = Math.max(6, Math.min(100, clamped * 100 * bias))
    bar.style.height = `${height}%`
  })
  container.dataset.active = clamped > 0.2 ? "true" : "false"
}

function updateMicIndicatorFromRms(rms) {
  const normalized = Math.max(0, Math.min(1, (rms - 0.005) / 0.06))
  setMicIndicatorLevel(normalized)
}

function getSelectedVoice() {
  const select = voiceSelect()
  return select?.value || "alloy"
}

function append(text) {
  const pane = document.getElementById("log")
  if (!pane) return
  pane.textContent += text
  pane.scrollTop = pane.scrollHeight
}

function logEvent(msg) {
  if (!msg || QUIET_EVENT_TYPES.has(msg.type)) return
  const pane = document.getElementById("events")
  if (!pane) return
  const summary = msg.event_id ? `${msg.type} (${msg.event_id})` : msg.type
  pane.textContent += `${summary}\n${JSON.stringify(msg, null, 2)}\n\n`
  pane.scrollTop = pane.scrollHeight
}

function getTextFromDelta(msg) {
  if (typeof msg.delta === "string") return msg.delta
  if (msg.delta && Array.isArray(msg.delta.text)) return msg.delta.text.join("")
  if (typeof msg.delta?.text === "string") return msg.delta.text
  if (Array.isArray(msg.text)) return msg.text.join("")
  if (typeof msg.text === "string") return msg.text
  return ""
}

function getTranscriptsFromItem(item) {
  if (!item) return []
  const transcripts = []
  if (typeof item.transcript === "string") {
    transcripts.push(item.transcript)
  }
  if (Array.isArray(item.content)) {
    item.content.forEach(entry => {
      if (typeof entry?.transcript === "string") {
        transcripts.push(entry.transcript)
      } else if (entry?.type === "input_text" && typeof entry.text === "string") {
        transcripts.push(entry.text)
      }
    })
  }
  if (item.input_audio_transcription && typeof item.input_audio_transcription.transcript === "string") {
    transcripts.push(item.input_audio_transcription.transcript)
  }
  return transcripts
}

function rememberConversationItem(id) {
  if (id) {
    conversationItemIds.add(id)
  }
}

function clearConversationItems() {
  if (!dc || dc.readyState !== "open" || conversationItemIds.size === 0) return
  conversationItemIds.forEach(id => {
    dc.send(JSON.stringify({
      type: "conversation.item.delete",
      item_id: id
    }))
  })
  conversationItemIds.clear()
}

function isAudioEnabled() {
  const toggle = audioToggle()
  return !!(toggle && toggle.checked)
}

function ensureRemoteAudioElement() {
  if (!remoteAudioEl) {
    remoteAudioEl = document.createElement("audio")
    remoteAudioEl.autoplay = true
    remoteAudioEl.style.display = "none"
    document.body.appendChild(remoteAudioEl)
  }
  return remoteAudioEl
}

function detachRemoteAudio() {
  if (remoteAudioEl) {
    remoteAudioEl.srcObject = null
  }
}

function attachRemoteAudioIfEnabled() {
  if (!remoteStream || !isAudioEnabled()) {
    detachRemoteAudio()
    return
  }
  const audioEl = ensureRemoteAudioElement()
  audioEl.srcObject = remoteStream
  audioEl.play().catch(() => {})
}

function sendAudioPreferenceUpdate() {
  if (!dc || dc.readyState !== "open") return
  const audioEnabled = isAudioEnabled()
  const voice = audioEnabled ? getSelectedVoice() : "none"
  const sessionUpdate = {
    instructions: audioEnabled
      ? "You are a voice-enabled assistant. Provide transcripts of the user and respond aloud."
      : "You are a text-based assistant. Return only the responses to what the user asks. No audio output.",
    voice
  }
  if (audioEnabled) {
    sessionUpdate.output_audio_format = "pcm16"
  }
  dc.send(JSON.stringify({
    type: "session.update",
    session: sessionUpdate
  }))
  attachRemoteAudioIfEnabled()
}

function requestTranscript() {
  if (!dc || dc.readyState !== "open" || awaitingResponse || currentResponseId || !sessionReady) return

  const now = Date.now()
  const rateLimitDelay = getRateLimitDelay()
  if (rateLimitDelay > 0) {
    scheduleRetry(rateLimitDelay)
    return
  }

  if (!hasRecentSpeech || !transcriptReady) {
    scheduleRetry(400)
    return
  }

  const timeSinceLast = now - lastRequestTs
  if (timeSinceLast < MIN_REQUEST_INTERVAL_MS) {
    scheduleRetry(MIN_REQUEST_INTERVAL_MS - timeSinceLast)
    return
  }

  lastRequestTs = now
  awaitingResponse = true
  responseHasText = false
  transcriptReady = false
  const audioEnabled = isAudioEnabled()
  const responsePayload = {
    type: "response.create",
    response: {
      modalities: audioEnabled ? ["text", "audio"] : ["text"],
      instructions: audioEnabled
        ? "Repeat the user's question, then answer it concisely. Provide both text and spoken audio."
        : "Repeat the user's question, then answer the question. No audio output."
    }
  }
  if (audioEnabled) {
    responsePayload.response.audio = {
      voice: getSelectedVoice(),
      format: "pcm16",
      sample_rate: OUTPUT_AUDIO_SAMPLE_RATE
    }
  }
  dc.send(JSON.stringify(responsePayload))
}

function scheduleRetry(delayMs = 400) {
  clearTimeout(retryTimer)
  retryTimer = setTimeout(() => {
    retryTimer = null
    requestTranscript()
  }, delayMs)
}

function finishResponse(success, retryDelayMs = null) {
  awaitingResponse = false
  if (success && responseHasText) {
    append("\n---\n")
  }
  responseHasText = false
  currentResponseId = null
  audioTranscriptBuffer = ""
  if (success) {
    clearConversationItems()
    hasRecentSpeech = false
    lastSpeechTs = 0
    requestTranscript()
  } else {
    transcriptReady = true
    scheduleRetry(retryDelayMs ?? 400)
  }
}

function parseRetryDelay(msg) {
  const message = msg?.response?.status_details?.error?.message || msg.error?.message || ""
  const clamp = value => Math.max(400, Math.min(value, 5 * 60 * 1000))

  const msMatch = message.match(/([0-9]+(?:\.[0-9]+)?)\s*ms/i)
  if (msMatch) {
    return clamp(parseFloat(msMatch[1]))
  }

  const compactMatch = message.match(/([0-9]+(?:\.[0-9]+)?)m([0-9]+(?:\.[0-9]+)?)s/i)
  if (compactMatch) {
    const minutes = parseFloat(compactMatch[1]) || 0
    const seconds = parseFloat(compactMatch[2]) || 0
    return clamp((minutes * 60 + seconds) * 1000)
  }

  const secondsMatch = message.match(/([0-9]+(?:\.[0-9]+)?)\s*s/i)
  if (secondsMatch) {
    return clamp(parseFloat(secondsMatch[1]) * 1000)
  }

  if (/requests per day/i.test(message)) {
    return clamp(5 * 60 * 1000)
  }

  return 1000
}

function noteRateLimitDelay(msg, delayMs) {
  if (!delayMs) return
  const limitName =
    msg?.response?.status_details?.error?.type ||
    msg?.response?.status_details?.error?.code ||
    msg?.error?.type ||
    msg?.error?.code
  if (!limitName) return
  const until = Date.now() + delayMs
  rateLimitBackoffs[limitName] = Math.max(rateLimitBackoffs[limitName] || 0, until)
}

function getRateLimitDelay() {
  const now = Date.now()
  let delay = 0
  Object.keys(rateLimitBackoffs).forEach(name => {
    const ts = rateLimitBackoffs[name]
    if (ts <= now) {
      delete rateLimitBackoffs[name]
    } else {
      delay = Math.max(delay, ts - now)
    }
  })
  return delay
}

function updateRateLimits(rateLimits) {
  if (!Array.isArray(rateLimits)) return
  const now = Date.now()
  const timersPane = document.getElementById("rate-limit-timers")
  if (timersPane) timersPane.innerHTML = ""
  rateLimits.forEach(limit => {
    if (!limit) return
    const name = limit.name || limit.type || "default"
    latestRateLimits[name] = limit
    const remaining = typeof limit.remaining === "number" ? limit.remaining : null
    const limitValue = typeof limit.limit === "number" ? limit.limit : null
    const resetSeconds = typeof limit.reset_seconds === "number" ? Math.max(0, limit.reset_seconds) : 0
    if (remaining !== null && limitValue !== null) {
      if (remaining <= 1 && resetSeconds > 0) {
        const until = now + resetSeconds * 1000
        rateLimitBackoffs[name] = Math.max(rateLimitBackoffs[name] || 0, until)
        append(`\n[limit] ${name} exhausted. Waiting ${resetSeconds.toFixed(2)}s\n`)
      } else if (limitValue > 0 && remaining / limitValue < 0.05) {
        append(`\n[limit] ${name} low: ${remaining}/${limitValue} (resets in ${resetSeconds.toFixed(2)}s)\n`)
      }
    }
    if (timersPane) {
      const entry = document.createElement("div")
      entry.id = `rl-${name}`
      timersPane.appendChild(entry)
      rateLimitTimers[name] = {
        element: entry,
        resetAt: now + resetSeconds * 1000,
        name,
        remaining,
        limit: limitValue
      }
    }
  })
  refreshRateLimitTimers()
}

function refreshRateLimitTimers() {
  const now = Date.now()
  Object.keys(rateLimitTimers).forEach(name => {
    const info = rateLimitTimers[name]
    if (!info.element) {
      delete rateLimitTimers[name]
      return
    }
    const timeLeft = Math.max(0, info.resetAt - now)
    const seconds = (timeLeft / 1000).toFixed(1)
    const remainingText = info.remaining !== null && info.limit !== null
      ? `${info.remaining}/${info.limit}`
      : "n/a"
    info.element.textContent = `${name}: resets in ${seconds}s (remaining ${remainingText})`
    if (timeLeft <= 0) {
      info.element.textContent = `${name}: ready`
      delete rateLimitTimers[name]
    }
  })
  if (Object.keys(rateLimitTimers).length > 0) {
    requestAnimationFrame(refreshRateLimitTimers)
  }
}

function startAudioMonitor() {
  if (!stream || audioContext) return
  audioContext = new AudioContext()
  const source = audioContext.createMediaStreamSource(stream)
  analyser = audioContext.createAnalyser()
  analyser.fftSize = 2048
  analyserBuffer = new Float32Array(analyser.fftSize)
  source.connect(analyser)

  const tick = () => {
    analyser.getFloatTimeDomainData(analyserBuffer)
    let sum = 0
    for (let i = 0; i < analyserBuffer.length; i++) {
      const sample = analyserBuffer[i]
      sum += sample * sample
    }
    const rms = Math.sqrt(sum / analyserBuffer.length)
    const now = performance.now()
    updateMicIndicatorFromRms(rms)

    if (rms > SPEECH_THRESHOLD) {
      hasRecentSpeech = true
      lastSpeechTs = now
      if (!awaitingResponse && !currentResponseId && sessionReady) {
        scheduleRetry(0)
      }
    } else if (hasRecentSpeech && now - lastSpeechTs > SILENCE_TIMEOUT_MS) {
      hasRecentSpeech = false
    }

    monitorHandle = requestAnimationFrame(tick)
  }

  tick()
}

function stopAudioMonitor() {
  if (monitorHandle) {
    cancelAnimationFrame(monitorHandle)
    monitorHandle = null
  }
  if (audioContext) {
    audioContext.close()
    audioContext = null
  }
  analyser = null
  analyserBuffer = null
  hasRecentSpeech = false
  lastSpeechTs = 0
  setMicIndicatorLevel(0)
}

async function startSession() {
  const token = document.getElementById("token").value
  if (!token) {
    alert("Missing API key")
    return
  }

  try {
    // microphone
    stream = await navigator.mediaDevices.getUserMedia({ audio: true })
    
    pc = new RTCPeerConnection({
      bundlePolicy: "max-bundle",
      iceServers: [
        { urls: "stun:stun.l.google.com:19302" },
        { urls: "stun:global.stun.twilio.com:3478" }
      ]
      })

    pc.ontrack = event => {
      remoteStream = event.streams[0]
      if (isAudioEnabled()) {
        const audioEl = ensureRemoteAudioElement()
        audioEl.srcObject = remoteStream
        audioEl.play().catch(() => {})
      } else {
        detachRemoteAudio()
      }
    }

    pc.addTrack(stream.getTracks()[0], stream)
    startAudioMonitor()

    pc.onconnectionstatechange = () => {
      if (!pc) return
      if (pc.connectionState === "failed" || pc.connectionState === "disconnected") {
        append("\n[connection lost]\n")
      }
    }

    // data channel for events
    dc = pc.createDataChannel("oai-events")

    dc.onmessage = evt => {
      const msg = JSON.parse(evt.data)
      logEvent(msg)

      if (msg.type === "session.created" || msg.type === "session.updated") {
        sessionReady = true
        scheduleRetry(0)
      } else if (msg.type === "response.created") {
        currentResponseId = msg.response?.id || msg.response_id || currentResponseId
      } else if (msg.type === "conversation.item.input_audio_transcription.completed") {
        rememberConversationItem(msg.item?.id || msg.item_id)
        const transcripts = []
        if (typeof msg.transcript === "string") transcripts.push(msg.transcript)
        transcripts.push(...getTranscriptsFromItem(msg.item))
        const text = transcripts.join(" ").trim()
        if (text) {
          append(`[heard] ${text}\n`)
          transcriptReady = true
          scheduleRetry(0)
        }
      } else if (
        msg.type === "response.output_text.delta" ||
        msg.type === "response.text.delta" ||
        msg.type === "response.audio_transcript.delta"
      ) {
        const deltaText = getTextFromDelta(msg)
        const audioDelta = msg.type === "response.audio_transcript.delta" ? msg.delta : ""
        const textToAppend = audioDelta || deltaText
        if (textToAppend) {
          responseHasText = true
          append(textToAppend)
          if (msg.type === "response.audio_transcript.delta") {
            audioTranscriptBuffer += textToAppend
          }
        }
      } else if (
        msg.type === "response.output_text.done" ||
        msg.type === "response.text.done" ||
        msg.type === "response.audio_transcript.done"
      ) {
        if (msg.type === "response.audio_transcript.done") {
          const finalText = msg.transcript || ""
          if (finalText && finalText.trim() === audioTranscriptBuffer.trim()) {
            append("\n---\n")
            responseHasText = false
            audioTranscriptBuffer = ""
            finishResponse(true)
            return
          } else if (finalText) {
            responseHasText = true
            append(finalText)
          }
        }
        if (responseHasText) append("\n")
      } else if (msg.type === "response.completed" || msg.type === "response.done") {
        if (msg.response_id && currentResponseId && msg.response_id !== currentResponseId) {
          return
        }
        const status = msg.response?.status
        if (status === "failed" || msg.response?.status_details?.type === "failed") {
          const errMsg = msg.response?.status_details?.error?.message || "Response failed"
          append(`\n[error] ${errMsg}\n`)
          const delay = parseRetryDelay(msg)
          noteRateLimitDelay(msg, delay)
          finishResponse(false, delay)
        } else {
          finishResponse(true)
        }
      } else if (msg.type === "response.failed") {
        append(`\n[error] ${msg.error?.message || "Response failed"}\n`)
        const delay = parseRetryDelay(msg)
        noteRateLimitDelay(msg, delay)
        finishResponse(false, delay)
      } else if (msg.type === "error" || msg.type === "response.error") {
        const errorText = msg.error?.message || evt.data || ""
        if (errorText.includes("Item with item_id not found")) {
          return
        }
        append(`\n[error] ${errorText}\n`)
        if (errorText.includes("active response")) {
          awaitingResponse = true
          return
        }
        const delay = parseRetryDelay(msg)
        noteRateLimitDelay(msg, delay)
        finishResponse(false, delay)
      } else if (msg.type === "rate_limits.updated") {
        updateRateLimits(msg.rate_limits)
      }
    }
    
    dc.onopen = () => {
      awaitingResponse = false
      sessionReady = false
      const audioEnabled = isAudioEnabled()
    const sessionUpdate = {
      instructions: audioEnabled
        ? "You are a voice-enabled assistant. Provide transcripts of the user and respond aloud."
        : "You are a transcription service. Return only the transcript of what the user says. No audio output.",
      voice: audioEnabled ? getSelectedVoice() : "none",
      turn_detection: {
        type: "server_vad",
        threshold: 0.5,
        prefix_padding_ms: 300,
        silence_duration_ms: 1200
      }
    }
    if (audioEnabled) {
      sessionUpdate.output_audio_format = "pcm16"
    }
    dc.send(JSON.stringify({
      type: "session.update",
      session: sessionUpdate
    }))
  }

    // WebRTC offer + wait for ICE so the API can reach us through NATs
    const offer = await pc.createOffer()
    await pc.setLocalDescription(offer)

    await new Promise((resolve, reject) => {
      if (!pc) return reject(new Error("Peer connection missing"))
      if (pc.iceGatheringState === "complete") return resolve()

      const checkState = () => {
        if (!pc) return reject(new Error("Peer connection closed"))
        if (pc.iceGatheringState === "complete") {
          pc.removeEventListener("icegatheringstatechange", checkState)
          return resolve()
        }
        if (pc.iceGatheringState === "failed") {
          pc.removeEventListener("icegatheringstatechange", checkState)
          return reject(new Error("ICE gathering failed"))
        }
      }
      pc.addEventListener("icegatheringstatechange", checkState)
      setTimeout(() => {
        if (!pc) return reject(new Error("Peer connection closed"))
        pc.removeEventListener("icegatheringstatechange", checkState)
        reject(new Error("Timed out waiting for ICE"))
      }, 7000)
    })

    const resp = await fetch(
      "https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17&mode=webrtc",
      {
        method: "POST",
        headers: {
          Authorization: `Bearer ${token}`,
          "Content-Type": "application/sdp",
          "OpenAI-Beta": "realtime=v1"
        },
        body: pc.localDescription.sdp
      }
    )

    if (!resp.ok) {
      throw new Error(`Realtime API request failed (${resp.status})`)
    }

    const answer = await resp.text()
    await pc.setRemoteDescription({ type: "answer", sdp: answer })
  } catch (err) {
    append(`\n[error] ${err.message}\n`)
    stopSession()
  }
}

function stopSession() {
  if (pc) pc.close()
  pc = null
  awaitingResponse = false
  responseHasText = false
  currentResponseId = null
  sessionReady = false
  hasRecentSpeech = false
  lastSpeechTs = 0
  lastRequestTs = 0
  transcriptReady = false
  audioTranscriptBuffer = ""
  clearTimeout(retryTimer)
  rateLimitTimers = {}
  conversationItemIds.clear()
  if (dc) {
    dc.close()
    dc = null
  }
  if (stream) {
    stream.getTracks().forEach(t => t.stop())
    stream = null
  }
  stopAudioMonitor()
  detachRemoteAudio()
  remoteStream = null
}

document.getElementById("start").onclick = startSession
document.getElementById("stop").onclick = stopSession
document.getElementById("enable-audio").addEventListener("change", sendAudioPreferenceUpdate)
document.getElementById("voice-select").addEventListener("change", () => {
  if (isAudioEnabled()) {
    sendAudioPreferenceUpdate()
  }
})
initMicIndicator()

</script>
</body>
</html>
